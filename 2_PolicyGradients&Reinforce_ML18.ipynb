{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "\n",
    "---\n",
    "A policy gradient attempts to train an agent without explicitly mapping the value for every state-action pair in an environment by taking small steps and updating the policy based on the reward associated with that step. The agent can receive a reward immediately for an action or the agent can receive the award at a later time such as the end of the episode.\n",
    "\n",
    "Formally, let's define a class of parameterized policies: $\\Pi = \\{\\pi_\\theta, \\theta \\in \\mathbb{R}^M \\}$. Now, for each policy, we define its value as:\n",
    "$$J(\\theta) = \\mathbb{E}\\big[\\sum_{t\\geq 0} \\gamma^t r_t | \\pi_\\theta \\big]$$\n",
    "\n",
    "Given this, our objective is to find an optimal policy $\\theta* = \\arg\\max_\\theta J(\\theta)$. We can achieve this by performing gradient ascent on policy parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE Algorithm\n",
    "**The Maths (Optional Read)**\n",
    "Let's try to derive the gradients of our objective function.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J(\\theta) &= \\mathbb{E}_{\\tau \\sim p(\\tau; \\theta)}\\big(r(\\tau)\\big)\\\\\n",
    "&= \\int_{\\tau} r(\\tau)p(\\tau;\\theta)d\\tau\n",
    "\\end{aligned}\n",
    "$$\n",
    "where, $r(\\tau)$ is the reward of a trajectory $\\tau = (s_0, a_0, r_0, s_1, \\dots)$. Now, let's differentiate this.\n",
    "$$ \\nabla_\\theta J(\\theta) = \\int_{\\tau} r(\\tau) \\nabla_\\theta p(\\tau; \\theta) d\\tau $$\n",
    "This is intractable as gradient of an expectation is problematic when p depends on $\\theta$.\n",
    "\n",
    "However, we can use a nice trick! \n",
    "$$\\nabla_\\theta p(\\tau; \\theta) = p(\\tau; \\theta) \\frac{\\nabla_\\theta p(\\tau; \\theta)}{p(\\tau; \\theta)} = p(\\tau; \\theta) \\nabla_\\theta \\log p(\\tau; \\theta)$$\n",
    "Now, we can re-write the gradients of our objective function as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta) &= \\int_\\tau \\Big(r(\\tau) \\nabla_\\theta \\log p(\\tau; \\theta)\\Big) p(\\tau; \\theta) d\\tau \\\\\n",
    "&= \\mathbb{E}_{\\tau \\sim p(\\tau; \\theta)} \\big[r(\\tau)\\nabla_\\theta \\log p(\\tau; \\theta)\\big]\n",
    "\\end{align}\n",
    "$$\n",
    "This term can now be estimated with Monte-Carlo sampling. However, the main challenge here is to compute this quantities without knowing the transition probabilities. Let's take a closer look at $p(\\tau; \\theta)$\n",
    "$$ p(\\tau; \\theta) = \\Pi_{t \\geq 0} p(s_{t+1}| s_t, a_t)\\pi_\\theta(a_t|s_t)$$\n",
    "$$ \\log p(\\tau; \\theta) = \\sum_{t \\geq 0} \\log p(s_{t+1}| s_t, a_t) + \\log \\pi_\\theta(a_t|s_t)$$\n",
    "and when differentiating, \n",
    "$$\\nabla_\\theta \\log p(\\tau;\\theta) = \\sum_{t \\geq 0} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$$\n",
    "Therefore when sampling a trajectory $\\tau$, we can estimate $J(\\theta)$ with,\n",
    "$$\\nabla_\\theta J(\\theta) \\approx \\sum_{t \\geq 0} r(\\tau) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$$\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "- If $r(\\tau)$ is high, push up the probabilities of the actions seen\n",
    "- If $r(\\tau)$ is low, push down the probabilities of the actions seen\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "1. Sample trajectories $\\{\\tau^i\\}$ from $\\pi_\\theta(a_t|s_t)$. (Run the policy -> forward pass through the network)\n",
    "2. Compute the estimated gradients, $\\nabla_\\theta J(\\theta) \\approx \\sum_{i} \\sum_{t \\geq 0} r(\\tau) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$\n",
    "3. Update the parameters, $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Learning to play Space Invaders\n",
    "<img src=\"images/space_invaders.jpg\" width=300>\n",
    "\n",
    "Now let's see how to apply the policy gradient to play an arcade game, space invaders, using OpenAI Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Working with OpenAI gym\n",
    "To get you acquanted with OpenAI's gym environment, here are some basic syntax:\n",
    "1. `env.reset()` - resets the environment\n",
    "2. `env.step(action)` - applies the given action in the environment which returns four values:<br>\n",
    "(a) `observation` (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game. <br>\n",
    "(b) `reward` (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward. <br>\n",
    "(c) `done` (boolean): whether it's time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.) <br>\n",
    "(d) `info` (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment's last state change). However, official evaluations of your agent are not allowed to use this for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "gym.logger.set_level(40) # suppress warnings (please remove if gives error)\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0) # set random seed\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the Architecture of the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(128,)\n",
      "action space: Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-ram-v0')\n",
    "env.seed(0)\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=128, h_size=256, a_size=6):\n",
    "        super(Policy, self).__init__()\n",
    "    \n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "        self.d = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.d(self.fc1(x)))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()    # Sampling from a multinomial distribution\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1: model saving...\n",
      "ep 2: model saving...\n",
      "ep 4: model saving...\n",
      "ep 6: model saving...\n",
      "Episode 100\tAverage Score: 192.25\n",
      "Episode 200\tAverage Score: 190.80\n",
      "Episode 300\tAverage Score: 211.85\n",
      "Episode 400\tAverage Score: 235.40\n"
     ]
    }
   ],
   "source": [
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)\n",
    "\n",
    "def reinforce(n_episodes=1000, max_t=1000, gamma=0.99, print_every=100):\n",
    "    max_iter_score = 0.0\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        discounts = [gamma**i for i in range(len(rewards)+1)]\n",
    "        R = sum([a*b for a,b in zip(discounts, rewards)])\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque) >= max_iter_score:\n",
    "            max_iter_score = np.mean(scores_deque)\n",
    "            print('ep %d: model saving...' % (i_episode))\n",
    "            torch.save(policy.state_dict(), 'pg_params.pkl')\n",
    "        \n",
    "    return scores\n",
    "    \n",
    "scores = reinforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot the Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Watch a Smart Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "env = gym.make('SpaceInvaders-ram-v0')\n",
    "policy.load_state_dict(torch.load('pg_params.pkl'))\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for t in range(100000):\n",
    "    action, _ = policy.act(state)\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
