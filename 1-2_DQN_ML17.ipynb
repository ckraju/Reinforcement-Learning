{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks\n",
    "\n",
    "In this lab we will train a Deep Q-Network (DQN) agent to play Pong task (Atari game) from OpenAI gym.\n",
    "\n",
    "## 1. Task\n",
    "The game of Pong is an excellent example of a simple RL task. In the ATARI 2600 version we’ll use you play as one of the paddles (the other is controlled by a decent AI) and you have to bounce the ball past the other player (I don’t really have to explain Pong, right?). On the low level the game works as follows: we receive an image frame (a 210x160x3 byte array (integers from 0 to 255 giving pixel values)) and we get to decide if we want to move the paddle UP or DOWN (i.e. a binary choice). After every single choice the game simulator executes the action and gives us a reward: Either a +1 reward if the ball went past the opponent, a -1 reward if we missed the ball, or 0 otherwise. And of course, our goal is to move the paddle so that we get lots of reward.\n",
    "\n",
    "<img src=images/pong.gif>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. From Q-Network to Deep Q-Network\n",
    "\n",
    "We saw that ordinary Q-network was able to barely perform as well as the Q-Table in a simple game environment, Deep Q-Networks are much more capable. In order to transform an ordinary Q-Network into a DQN we will be making the following improvements:\n",
    "- Going from a single-layer network to a multi-layer convolutional network.\n",
    "- Implementing Experience Replay, which will allow our network to train itself using stored memories from it’s experience.\n",
    "- Utilizing a second “target” network, which we will use to compute target Q-values during our updates.\n",
    "\n",
    "It was these three innovations that allowed the [Google DeepMind team to achieve superhuman performance on dozens of Atari games using their DQN agent](http://www.davidqiu.com:8888/research/nature14236.pdf). We will be walking through each individual improvement.\n",
    "\n",
    "### Addition 1: Convolutional Layers\n",
    "Since our agent is going to be learning to play video games, it has to be able to make sense of the game’s screen output in a way that is at least similar to how humans or other intelligent animals are able to. Instead of considering each pixel independently, convolutional layers allow us to consider regions of an image, and maintain spatial relationships between the objects on the screen as we send information up to higher levels of the network. \n",
    "\n",
    "### Addition 2: Experience Replay\n",
    "The second major addition to make DQNs work is Experience Replay. The basic idea is that by storing an agent’s experiences, and then randomly drawing batches of them to train the network, we can more robustly learn to perform well in the task. By keeping the experiences we draw random, we prevent the network from only learning about what it is immediately doing in the environment, and allow it to learn from a more varied array of past experiences. Each of these experiences are stored as a tuple of $\\text{<state, action, reward, next_state>}$. The Experience Replay buffer stores a fixed number of recent memories, and as new ones come in, old ones are removed. When the time comes to train, we simply draw a uniform batch of random memories from the buffer, and train our network with them. For our DQN, we have a helper function named `ReplayBuffer` which implements this.\n",
    "\n",
    "### Addition 3: Separate Target Network\n",
    "The third major addition to the DQN that makes it unique is the utilization of a second network during the training procedure. This second network is used to generate the target-Q values that will be used to compute the loss for every action during training. Why not use just use one network for both estimations? The issue is that at every step of training, the Q-network’s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated Q-values. In order to mitigate that risk, the target network’s weights are fixed, and only periodically or slowly updated to the primary Q-networks values. In this way training can proceed in a more stable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "from utils.gym import get_env, get_wrapper_by_name\n",
    "from utils.schedule import ConstantSchedule, PiecewiseSchedule, LinearSchedule\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Define Global Variables #\n",
    "###########################\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "REPLAY_BUFFER_SIZE = 1000000\n",
    "LEARNING_STARTS = 50000\n",
    "LEARNING_FREQ = 4\n",
    "FRAME_HISTORY_LEN = 4\n",
    "TARGET_UPDATE_FREQ = 10000\n",
    "LEARNING_RATE = 0.00025\n",
    "ALPHA = 0.95\n",
    "EPS = 0.01\n",
    "PONG = 3\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "class Variable(autograd.Variable):\n",
    "    def __init__(self, data, *args, **kwargs):\n",
    "        if USE_CUDA:\n",
    "            data = data.cuda()\n",
    "        super(Variable, self).__init__(data, *args, **kwargs)\n",
    "\n",
    "\"\"\"\n",
    "    OptimizerSpec containing following attributes\n",
    "        constructor: The optimizer constructor ex: RMSprop\n",
    "        kwargs: {Dict} arguments for constructing optimizer\n",
    "\"\"\"\n",
    "OptimizerSpec = namedtuple(\"OptimizerSpec\", [\"constructor\", \"kwargs\"])\n",
    "\n",
    "Statistic = {\n",
    "    \"mean_episode_rewards\": [],\n",
    "    \"best_mean_episode_rewards\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# Define DQN #\n",
    "##############\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, num_actions=18):\n",
    "        \"\"\"\n",
    "        Initialize a deep Q-learning network as described in\n",
    "        https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "        Arguments:\n",
    "            in_channels: number of channel of input.\n",
    "                i.e The number of most recent frames stacked together as describe in the paper\n",
    "            num_actions: number of action-value to output, one-to-one correspondence to action in game.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "        return self.fc5(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "## TRAINING FUNCTION ##\n",
    "#######################\n",
    "\n",
    "def dqn_learing(\n",
    "    env,\n",
    "    q_func,\n",
    "    optimizer_spec,\n",
    "    exploration,\n",
    "    stopping_criterion=None,\n",
    "    replay_buffer_size=1000000,\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    learning_starts=50000,\n",
    "    learning_freq=4,\n",
    "    frame_history_len=4,\n",
    "    target_update_freq=10000\n",
    "    ):\n",
    "\n",
    "    \"\"\"Run Deep Q-learning algorithm.\n",
    "\n",
    "    You can specify your own convnet using q_func.\n",
    "\n",
    "    All schedules are w.r.t. total number of steps taken in the environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.Env\n",
    "        gym environment to train on.\n",
    "    q_func: function\n",
    "        Model to use for computing the q function. It should accept the\n",
    "        following named arguments:\n",
    "            input_channel: int\n",
    "                number of channel of input.\n",
    "            num_actions: int\n",
    "                number of actions\n",
    "    optimizer_spec: OptimizerSpec\n",
    "        Specifying the constructor and kwargs, as well as learning rate schedule\n",
    "        for the optimizer\n",
    "    exploration: Schedule (defined in utils.schedule)\n",
    "        schedule for probability of chosing random action.\n",
    "    stopping_criterion: (env) -> bool\n",
    "        should return true when it's ok for the RL algorithm to stop.\n",
    "        takes in env and the number of steps executed so far.\n",
    "    replay_buffer_size: int\n",
    "        How many memories to store in the replay buffer.\n",
    "    batch_size: int\n",
    "        How many transitions to sample each time experience is replayed.\n",
    "    gamma: float\n",
    "        Discount Factor\n",
    "    learning_starts: int\n",
    "        After how many environment steps to start replaying experiences\n",
    "    learning_freq: int\n",
    "        How many steps of environment to take between every experience replay\n",
    "    frame_history_len: int\n",
    "        How many past frames to include as input to the model.\n",
    "    target_update_freq: int\n",
    "        How many experience replay rounds (not steps!) to perform between\n",
    "        each update to the target Q network\n",
    "    \"\"\"\n",
    "    assert type(env.observation_space) == gym.spaces.Box\n",
    "    assert type(env.action_space)      == gym.spaces.Discrete\n",
    "\n",
    "    ###############\n",
    "    # BUILD MODEL #\n",
    "    ###############\n",
    "\n",
    "    if len(env.observation_space.shape) == 1:\n",
    "        # This means we are running on low-dimensional observations (e.g. RAM)\n",
    "        input_arg = env.observation_space.shape[0]\n",
    "    else:\n",
    "        img_h, img_w, img_c = env.observation_space.shape\n",
    "        input_arg = frame_history_len * img_c\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Construct an epilson greedy policy with given exploration schedule\n",
    "    def select_epilson_greedy_action(model, obs, t):\n",
    "        sample = random.random()\n",
    "        eps_threshold = exploration.value(t)\n",
    "        if sample > eps_threshold:\n",
    "            obs = torch.from_numpy(obs).type(dtype).unsqueeze(0) / 255.0\n",
    "            # Use volatile = True if variable is only used in inference mode, i.e. don’t save the history\n",
    "            return model(Variable(obs, volatile=True)).data.max(1)[1].cpu()\n",
    "        else:\n",
    "            return torch.IntTensor([[random.randrange(num_actions)]])\n",
    "\n",
    "    # Initialize target q function and q function\n",
    "    Q = q_func(input_arg, num_actions).type(dtype)\n",
    "    target_Q = q_func(input_arg, num_actions).type(dtype)\n",
    "\n",
    "    # Construct Q network optimizer function\n",
    "    optimizer = optimizer_spec.constructor(Q.parameters(), **optimizer_spec.kwargs)\n",
    "\n",
    "    # Construct the replay buffer\n",
    "    replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)\n",
    "\n",
    "    ###############\n",
    "    # RUN ENV     #\n",
    "    ###############\n",
    "    num_param_updates = 0\n",
    "    mean_episode_reward = -float('nan')\n",
    "    best_mean_episode_reward = -float('inf')\n",
    "    last_obs = env.reset()\n",
    "    LOG_EVERY_N_STEPS = 10000\n",
    "\n",
    "    for t in count():\n",
    "        ### Check stopping criterion\n",
    "        if stopping_criterion is not None and stopping_criterion(env):\n",
    "            break\n",
    "\n",
    "        ### Step the env and store the transition\n",
    "        # Store lastest observation in replay memory and last_idx can be used to store action, reward, done\n",
    "        last_idx = replay_buffer.store_frame(last_obs)\n",
    "        # encode_recent_observation will take the latest observation\n",
    "        # that you pushed into the buffer and compute the corresponding\n",
    "        # input that should be given to a Q network by appending some\n",
    "        # previous frames.\n",
    "        recent_observations = replay_buffer.encode_recent_observation()\n",
    "\n",
    "        # Choose random action if not yet start learning\n",
    "        if t > learning_starts:\n",
    "            action = select_epilson_greedy_action(Q, recent_observations, t)[0, 0]\n",
    "        else:\n",
    "            action = random.randrange(num_actions)\n",
    "        # Advance one step\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        # clip rewards between -1 and 1\n",
    "        reward = max(-1.0, min(reward, 1.0))\n",
    "        # Store other info in replay memory\n",
    "        replay_buffer.store_effect(last_idx, action, reward, done)\n",
    "        # Resets the environment when reaching an episode boundary.\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "        last_obs = obs\n",
    "\n",
    "        ### Perform experience replay and train the network.\n",
    "        # Note that this is only done if the replay buffer contains enough samples\n",
    "        # for us to learn something useful -- until then, the model will not be\n",
    "        # initialized and random actions should be taken\n",
    "        if (t > learning_starts and\n",
    "                t % learning_freq == 0 and\n",
    "                replay_buffer.can_sample(batch_size)):\n",
    "            # Use the replay buffer to sample a batch of transitions\n",
    "            # Note: done_mask[i] is 1 if the next state corresponds to the end of an episode,\n",
    "            # in which case there is no Q-value at the next state; at the end of an\n",
    "            # episode, only the current state reward contributes to the target\n",
    "            obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = replay_buffer.sample(batch_size)\n",
    "            # Convert numpy nd_array to torch variables for calculation\n",
    "            obs_batch = Variable(torch.from_numpy(obs_batch).type(dtype) / 255.0)\n",
    "            act_batch = Variable(torch.from_numpy(act_batch).long())\n",
    "            rew_batch = Variable(torch.from_numpy(rew_batch))\n",
    "            next_obs_batch = Variable(torch.from_numpy(next_obs_batch).type(dtype) / 255.0)\n",
    "            not_done_mask = Variable(torch.from_numpy(1 - done_mask)).type(dtype)\n",
    "\n",
    "            if USE_CUDA:\n",
    "                act_batch = act_batch.cuda()\n",
    "                rew_batch = rew_batch.cuda()\n",
    "\n",
    "            # Compute current Q value, q_func takes only state and output value for every state-action pair\n",
    "            # We choose Q based on action taken.\n",
    "            current_Q_values = Q(obs_batch).gather(1, act_batch.unsqueeze(1))\n",
    "            # Compute next Q value based on which action gives max Q values\n",
    "            # Detach variable from the current graph since we don't want gradients for next Q to propagated\n",
    "            next_max_q = target_Q(next_obs_batch).detach().max(1)[0]\n",
    "            next_Q_values = not_done_mask * next_max_q\n",
    "            # Compute the target of the current Q values\n",
    "            target_Q_values = rew_batch + (gamma * next_Q_values)\n",
    "            # Compute Bellman error\n",
    "            bellman_error = target_Q_values - current_Q_values\n",
    "            # clip the bellman error between [-1 , 1]\n",
    "            clipped_bellman_error = bellman_error.clamp(-1, 1)\n",
    "            # Note: clipped_bellman_delta * -1 will be right gradient\n",
    "            d_error = clipped_bellman_error * -1.0\n",
    "            # Clear previous gradients before backward pass\n",
    "            optimizer.zero_grad()\n",
    "            # run backward pass\n",
    "            current_Q_values.backward(d_error.data.unsqueeze(1))\n",
    "\n",
    "            # Perfom the update\n",
    "            optimizer.step()\n",
    "            num_param_updates += 1\n",
    "\n",
    "            # Periodically update the target network by Q network to target Q network\n",
    "            if num_param_updates % target_update_freq == 0:\n",
    "                target_Q.load_state_dict(Q.state_dict())\n",
    "\n",
    "        ### 4. Log progress and keep track of statistics\n",
    "        episode_rewards = get_wrapper_by_name(env, \"Monitor\").get_episode_rewards()\n",
    "        if len(episode_rewards) > 0:\n",
    "            mean_episode_reward = np.mean(episode_rewards[-100:])\n",
    "        if len(episode_rewards) > 100:\n",
    "            best_mean_episode_reward = max(best_mean_episode_reward, mean_episode_reward)\n",
    "\n",
    "        Statistic[\"mean_episode_rewards\"].append(mean_episode_reward)\n",
    "        Statistic[\"best_mean_episode_rewards\"].append(best_mean_episode_reward)\n",
    "\n",
    "        if t % LOG_EVERY_N_STEPS == 0 and t > learning_starts:\n",
    "            print(\"Timestep %d\" % (t,))\n",
    "            print(\"mean reward (100 episodes) %f\" % mean_episode_reward)\n",
    "            print(\"best mean reward %f\" % best_mean_episode_reward)\n",
    "            print(\"episodes %d\" % len(episode_rewards))\n",
    "            print(\"exploration %f\" % exploration.value(t))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Dump statistics to pickle\n",
    "            # with open('statistics.pkl', 'wb') as f:\n",
    "            #     pickle.dump(Statistic, f)\n",
    "            #     print(\"Saved to %s\" % 'results/statistics.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-14 13:54:34,773] Making new env: PongNoFrameskip-v4\n",
      "[2017-07-14 13:54:35,581] Clearing 14 monitor files from previous run (because force=True was provided)\n",
      "[2017-07-14 13:54:37,323] Starting new video recorder writing to /users/aditya.a/cvit_schools/cvitmlss17/lab5/results/pong/openaigym.video.0.25657.video000000.mp4\n",
      "[2017-07-14 13:54:42,063] Starting new video recorder writing to /users/aditya.a/cvit_schools/cvitmlss17/lab5/results/pong/openaigym.video.0.25657.video000001.mp4\n",
      "[2017-07-14 13:54:55,846] Starting new video recorder writing to /users/aditya.a/cvit_schools/cvitmlss17/lab5/results/pong/openaigym.video.0.25657.video000008.mp4\n",
      "[2017-07-14 13:55:30,719] Starting new video recorder writing to /users/aditya.a/cvit_schools/cvitmlss17/lab5/results/pong/openaigym.video.0.25657.video000027.mp4\n",
      "[2017-07-14 13:57:23,090] Starting new video recorder writing to /users/aditya.a/cvit_schools/cvitmlss17/lab5/results/pong/openaigym.video.0.25657.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 60000\n",
      "mean reward (100 episodes) -20.338462\n",
      "best mean reward -inf\n",
      "episodes 65\n",
      "exploration 0.946000\n",
      "Saved to results/statistics.pkl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-37a9b1fc40d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mlearning_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_FREQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mframe_history_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFRAME_HISTORY_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtarget_update_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARGET_UPDATE_FREQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6f537bce4174>\u001b[0m in \u001b[0;36mdqn_learing\u001b[0;34m(env, q_func, optimizer_spec, exploration, stopping_criterion, replay_buffer_size, batch_size, gamma, learning_starts, learning_freq, frame_history_len, target_update_freq)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mrew_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrew_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mnext_obs_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_obs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mnot_done_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdone_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mUSE_CUDA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Libraries/miniconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Libraries/miniconda3/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdest_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdest_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Libraries/miniconda3/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##########\n",
    "## MAIN ##\n",
    "##########\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    ## Set Gym\n",
    "    # Get Atari games.\n",
    "    atari = gym.benchmark_spec('Atari40M')\n",
    "\n",
    "    # Change the index to select a different game.\n",
    "    task = atari.tasks[PONG]\n",
    "    num_timesteps = task.max_timesteps\n",
    "    \n",
    "    ## Run training\n",
    "    seed = 0        # Use a seed of zero\n",
    "    env = get_env(task, seed)\n",
    "    \n",
    "    def stopping_criterion(env):\n",
    "        # notice that here t is the number of steps of the wrapped env,\n",
    "        # which is different from the number of steps in the underlying env\n",
    "        return get_wrapper_by_name(env, \"Monitor\").get_total_steps() >= num_timesteps\n",
    "    \n",
    "    optimizer_spec = OptimizerSpec(\n",
    "        constructor=optim.RMSprop,\n",
    "        kwargs=dict(lr=LEARNING_RATE, alpha=ALPHA, eps=EPS),\n",
    "    )\n",
    "    \n",
    "    exploration_schedule = LinearSchedule(1000000, 0.1)\n",
    "    \n",
    "    dqn_learing(\n",
    "        env=env,\n",
    "        q_func=DQN,\n",
    "        optimizer_spec=optimizer_spec,\n",
    "        exploration=exploration_schedule,\n",
    "        stopping_criterion=stopping_criterion,\n",
    "        replay_buffer_size=REPLAY_BUFFER_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        gamma=GAMMA,\n",
    "        learning_starts=LEARNING_STARTS,\n",
    "        learning_freq=LEARNING_FREQ,\n",
    "        frame_history_len=FRAME_HISTORY_LEN,\n",
    "        target_update_freq=TARGET_UPDATE_FREQ,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(Statistic[\"mean_episode_rewards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(Statistic[\"best_mean_episode_rewards\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Excercise\n",
    "\n",
    "0. Observe the performance of the DQN at various training phases in the results folder.\n",
    "1. Change the hyperparameters, and observe the performance of DQN.\n",
    "2. Use different `Schedules` (`Constant`, `Piecewise`) and compare them with `LinearSchedule`. (Codes for these are already included, you just need to replace `LinearSchedule` with them.\n",
    "3. Apply different exploration strategies like `greedy`, `random` and check perfromance of the DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement\n",
    "\n",
    "The helper functions have been adapted/copied from Berkely deep learning course [homework 3](https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
